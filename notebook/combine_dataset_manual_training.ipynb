{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90043c42-e5a1-42c6-99ad-74e2c81bebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3582c60b-6e8e-4933-afdc-6ddc4043dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if not \"..\" in sys.path:\n",
    "    sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23051e7-9e5d-4344-b3ad-a5b2a454035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 03:56:09.928808: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-15 03:56:12.334784: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-15 03:56:12.335426: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-15 03:56:12.363863: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-15 03:56:12.994669: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-15 03:56:22.599764: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from lit_modules.litdatamodules.lit_imagenet import LitImageNetDataModule\n",
    "from lit_modules.litdatamodules.lit_lamem import LitLaMemDataModule\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.utilities.types import TRAIN_DATALOADERS\n",
    "from lightning.pytorch.utilities import CombinedLoader\n",
    "from lightning.pytorch.callbacks import (\n",
    "    DeviceStatsMonitor,\n",
    "    StochasticWeightAveraging,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lit_modules import LitResNet50\n",
    "from lightning import Trainer\n",
    "\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4153b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitCombineDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_imagenet: str,\n",
    "        meta_path_imagenet: str,\n",
    "        root_lamem: str,\n",
    "        num_workers: int = 10,\n",
    "        batch_size: int = 32,\n",
    "        mode: str = \"min_size\",\n",
    "        desired_image_size: int = 224,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(\"batch_size\")\n",
    "        self.save_hyperparameters(\"desired_image_size\")\n",
    "        self.mode = mode\n",
    "\n",
    "        self.data_modules = {\n",
    "            \"regression\": LitLaMemDataModule(\n",
    "                root=root_lamem,\n",
    "                num_workers=num_workers,\n",
    "                batch_size=batch_size,\n",
    "                desired_image_size=desired_image_size,\n",
    "            ),\n",
    "            \"classification\": LitImageNetDataModule(\n",
    "                root=root_imagenet,\n",
    "                meta_path=meta_path_imagenet,\n",
    "                num_workers=num_workers,\n",
    "                batch_size=batch_size,\n",
    "                desired_image_size=desired_image_size,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "\n",
    "        for key, value in self.data_modules.items():\n",
    "            value.setup(stage)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        data_loaders = {\n",
    "            \"regression\": self.data_modules[\"regression\"].train_dataloader(),\n",
    "            \"classification\": self.data_modules[\"classification\"].train_dataloader(),\n",
    "        }\n",
    "\n",
    "        return CombinedLoader(data_loaders, mode=self.mode)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        data_loaders = {\n",
    "            \"regression\": self.data_modules[\"regression\"].val_dataloader(),\n",
    "            \"classification\": self.data_modules[\"classification\"].val_dataloader(),\n",
    "        }\n",
    "\n",
    "        return CombinedLoader(data_loaders, mode=self.mode)\n",
    "\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load a pre-trained ResNet-50 model\n",
    "        self.resnet = resnet50(weights=None)\n",
    "        self.linear = nn.Linear(1000, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, task):\n",
    "        x = self.resnet(x)\n",
    "\n",
    "        if task == \"regression\":\n",
    "            x = self.linear(x)\n",
    "            x = self.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f1f6bb-79f9-456d-a747-a178b3a80a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders():\n",
    "    root_imagenet = \"/datashare/ImageNet/ILSVRC2012\"\n",
    "    meta_path_imagenet = (\n",
    "        \"/home/soroush1/projects/def-kohitij/soroush1/pretrain-imagenet/data/ImageNet\"\n",
    "    )\n",
    "    root_lamem = \"/home/soroush1/projects/def-kohitij/soroush1/pretrain-imagenet/data/lamem/lamem_images/lamem\"\n",
    "    batch_size = 1\n",
    "    ds = LitCombineDataModule(\n",
    "        root_imagenet=root_imagenet,\n",
    "        meta_path_imagenet=meta_path_imagenet,\n",
    "        root_lamem=root_lamem,\n",
    "        batch_size=64,\n",
    "        mode=\"min_size\",\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    ds.setup(\"\")\n",
    "\n",
    "    train_loader = ds.train_dataloader()\n",
    "    val_loader = ds.val_dataloader()\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def calculate_initial_task_losses(model, train_loader, criterion, device):\n",
    "    with torch.no_grad():\n",
    "        overfit_batch = []\n",
    "        for i, (batch, batch_id, dataloader_idx) in enumerate(train_loader):\n",
    "            overfit_batch.append([batch, batch_id, dataloader_idx])\n",
    "\n",
    "            if i == 0:\n",
    "                break\n",
    "\n",
    "        for batch, batch_id, dataloader_idx in tqdm(\n",
    "            overfit_batch, total=len(overfit_batch)\n",
    "        ):\n",
    "            init_task_losses = []\n",
    "            for key, value in batch.items():\n",
    "                x, y = value\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                outputs = model(x, key)\n",
    "                loss = criterion[key][\"loss\"](outputs.squeeze(), y)\n",
    "                init_task_losses.append(loss)\n",
    "\n",
    "    print(f\"{init_task_losses = }\")\n",
    "\n",
    "    return init_task_losses\n",
    "\n",
    "\n",
    "def grad_norm_step(model, loss_weights, task_losses, initial_task_losses, alpha=0.16):\n",
    "    \"\"\"\n",
    "    Perform a single step of the GradNorm algorithm.\n",
    "    - model: the multitask model\n",
    "    - loss_weights: a list of the weights for each task's loss\n",
    "    - task_losses: a list of the current losses for each task\n",
    "    - alpha: GradNorm hyperparameter controlling the strength of the gradients normalization\n",
    "    \"\"\"\n",
    "    # Calculate the gradients for each task's loss with respect to the loss weights\n",
    "    # This requires keeping the loss weights as parameters to allow gradient computation\n",
    "\n",
    "    weighted_losses = [\n",
    "        loss_weights[i] * task_losses[i] for i in range(len(task_losses))\n",
    "    ]\n",
    "    total_loss = sum(weighted_losses)\n",
    "    total_loss.backward(retain_graph=True)  # Retain graph for multiple backward passes\n",
    "\n",
    "    # Calculate the gradient norms for each task\n",
    "    W = list(model.parameters())[0]  # Assuming W is the shared parameter(s)\n",
    "    grad_norms = [torch.norm(W.grad * loss_weights[i]) for i in range(len(task_losses))]\n",
    "\n",
    "    # Normalize the gradient norms\n",
    "    mean_grad_norm = torch.mean(torch.stack(grad_norms))\n",
    "    grad_norms_normalized = [grad_norm / mean_grad_norm for grad_norm in grad_norms]\n",
    "\n",
    "    # Update the loss weights\n",
    "    loss_ratios = [\n",
    "        task_losses[i] / initial_task_losses[i] for i in range(len(task_losses))\n",
    "    ]\n",
    "    inverse_training_rates = [loss_ratio**alpha for loss_ratio in loss_ratios]\n",
    "    mean_inverse_training_rate = torch.mean(torch.tensor(inverse_training_rates))\n",
    "    new_loss_weights = [\n",
    "        loss_weights[i] * (inverse_training_rate / mean_inverse_training_rate)\n",
    "        for i, inverse_training_rate in enumerate(inverse_training_rates)\n",
    "    ]\n",
    "\n",
    "    # Reset gradients for next optimization step\n",
    "    model.zero_grad()\n",
    "\n",
    "    return new_loss_weights\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    train_loader,\n",
    "    model,\n",
    "    loss_weights,\n",
    "    initial_task_losses,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    model_specification,\n",
    "    epoch_num: int,\n",
    "    step: bool = True,\n",
    "    epoch: bool = True,\n",
    "    overfit: bool = False,\n",
    "):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    overfit_batch = []\n",
    "    for i, (batch, batch_id, dataloader_idx) in enumerate(train_loader):\n",
    "        overfit_batch.append([batch, batch_id, dataloader_idx])\n",
    "\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "    if overfit:\n",
    "        train_loader = overfit_batch\n",
    "\n",
    "    for i, (batch, batch_idx, dataloader_idx) in tqdm(\n",
    "        enumerate(train_loader), total=len(train_loader)\n",
    "    ):\n",
    "        task_losses = []\n",
    "        optimizer.zero_grad()\n",
    "        for key, value in batch.items():\n",
    "            x, y = value\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = model(x, key)\n",
    "            loss = criterion[key][\"loss\"](outputs.squeeze(), y)\n",
    "            model_specification[f\"tr_{key}_loss\"].append(loss.item())\n",
    "\n",
    "            if key == \"classification\":\n",
    "                err_top1 = 1 - criterion[key][\"top1\"](outputs.squeeze(), y)\n",
    "                err_top5 = 1 - criterion[key][\"top5\"](outputs.squeeze(), y)\n",
    "                model_specification[\"tr_err_top1\"].append(err_top1.item())\n",
    "                model_specification[\"tr_err_top5\"].append(err_top5.item())\n",
    "\n",
    "            task_losses.append(loss)\n",
    "\n",
    "        # GradNorm step to adjust gradients and update loss weights\n",
    "        new_loss_weights = grad_norm_step(\n",
    "            model, loss_weights, task_losses, initial_task_losses\n",
    "        )\n",
    "\n",
    "        model_specification[f\"tr_total_loss\"].append(sum(task_losses).item())\n",
    "        # Apply new loss weights and compute total loss for parameter update\n",
    "        weighted_losses = [\n",
    "            new_loss_weights[i] * task_losses[i] for i in range(len(task_losses))\n",
    "        ]\n",
    "        model_specification[f\"tr_weight_loss\"].append(sum(weighted_losses).item())\n",
    "\n",
    "        total_loss = sum(weighted_losses)\n",
    "        # Backward pass and optimize\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        writer.add_scalar(\n",
    "            \"Learning Rate\", current_lr, epoch * len(train_loader) + batch_idx\n",
    "        )\n",
    "        if step:\n",
    "            # Log metrics to TensorBoard\n",
    "            for key, values in model_specification.items():\n",
    "                writer.add_scalar(\n",
    "                    f\"Training_step/{key}\",\n",
    "                    values[i],\n",
    "                    epoch_num * len(train_loader) + batch_idx,\n",
    "                )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch:\n",
    "        # Log metrics to TensorBoard\n",
    "        for key, values in model_specification.items():\n",
    "            writer.add_scalar(\n",
    "                f\"Training_epoch/{key}\", sum(values) / len(values), epoch_num\n",
    "            )\n",
    "\n",
    "\n",
    "def val_loop(\n",
    "    val_loader,\n",
    "    model,\n",
    "    criterion,\n",
    "    device,\n",
    "    model_specification,\n",
    "    epoch_num: int,\n",
    "    step: bool = True,\n",
    "    epoch: bool = True,\n",
    "    overfit: bool = False,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    overfit_batch = []\n",
    "    for i, (batch, batch_id, dataloader_idx) in enumerate(val_loader):\n",
    "        overfit_batch.append([batch, batch_id, dataloader_idx])\n",
    "\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "    if overfit:\n",
    "        val_loader = overfit_batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch, batch_idx, dataloader_idx) in tqdm(\n",
    "            enumerate(val_loader), total=len(val_loader)\n",
    "        ):\n",
    "            task_losses = []\n",
    "            for key, value in batch.items():\n",
    "                x, y = value\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                outputs = model(x, key)\n",
    "                loss = criterion[key][\"loss\"](outputs.squeeze(), y)\n",
    "                model_specification[f\"val_{key}_loss\"].append(loss.item())\n",
    "\n",
    "                if key == \"classification\":\n",
    "                    err_top1 = 1 - criterion[key][\"top1\"](outputs.squeeze(), y)\n",
    "                    err_top5 = 1 - criterion[key][\"top5\"](outputs.squeeze(), y)\n",
    "                    model_specification[\"val_err_top1\"].append(err_top1.item())\n",
    "                    model_specification[\"val_err_top5\"].append(err_top5.item())\n",
    "\n",
    "            if step:\n",
    "                # Log metrics to TensorBoard\n",
    "                for key, values in model_specification.items():\n",
    "                    writer.add_scalar(\n",
    "                        f\"Validation_step/{key}\",\n",
    "                        values[i],\n",
    "                        epoch_num * len(val_loader) + batch_idx,\n",
    "                    )\n",
    "\n",
    "        if epoch:\n",
    "            # Log metrics to TensorBoard\n",
    "            for key, values in model_specification.items():\n",
    "                writer.add_scalar(\n",
    "                    f\"Validation_epoch/{key}\", sum(values) / len(values), epoch_num\n",
    "                )\n",
    "\n",
    "\n",
    "def get_next_version_number(base_dir, experiment_name):\n",
    "    experiment_path = os.path.join(base_dir, experiment_name)\n",
    "    if not os.path.exists(experiment_path):\n",
    "        os.makedirs(experiment_path)\n",
    "        return 0\n",
    "\n",
    "    existing_versions = [\n",
    "        d\n",
    "        for d in os.listdir(experiment_path)\n",
    "        if os.path.isdir(os.path.join(experiment_path, d))\n",
    "    ]\n",
    "    if not existing_versions:\n",
    "        return 0\n",
    "\n",
    "    highest_version = max([int(v.split(\"_\")[-1]) for v in existing_versions])\n",
    "    return highest_version + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb011df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch = 8\n",
    "device = \"cuda:0\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "train_loader, val_loader = get_dataloaders()\n",
    "model = ResNet50()\n",
    "model.to(device)\n",
    "\n",
    "# Assuming initial_task_losses is a pre-computed list of initial losses for each task\n",
    "# loss_weights should be initialized as torch parameters to allow gradient updates\n",
    "loss_weights = [\n",
    "    torch.nn.Parameter(torch.ones(1, requires_grad=True)).cuda() for _ in range(2)\n",
    "]  # 2 tasks\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(), betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-3\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "classes = 1000\n",
    "criterion = {\n",
    "    \"regression\": {\"loss\": nn.MSELoss()},\n",
    "    \"classification\": {\n",
    "        \"loss\": nn.CrossEntropyLoss(),\n",
    "        \"top1\": Accuracy(task=\"multiclass\", num_classes=classes, top_k=1).to(device),\n",
    "        \"top5\": Accuracy(task=\"multiclass\", num_classes=classes, top_k=5).to(device),\n",
    "    },\n",
    "}\n",
    "\n",
    "initial_task_losses = calculate_initial_task_losses(\n",
    "    model, train_loader, criterion, device\n",
    ")\n",
    "\n",
    "tr_model_specification = {\n",
    "    \"tr_regression_loss\": [],\n",
    "    \"tr_classification_loss\": [],\n",
    "    \"tr_total_loss\": [],\n",
    "    \"tr_weight_loss\": [],\n",
    "    \"tr_err_top1\": [],\n",
    "    \"tr_err_top5\": [],\n",
    "}\n",
    "\n",
    "val_model_specification = {\n",
    "    \"val_regression_loss\": [],\n",
    "    \"val_classification_loss\": [],\n",
    "    \"val_err_top1\": [],\n",
    "    \"val_err_top5\": [],\n",
    "}\n",
    "\n",
    "# Base directory where all TensorBoard logs are stored\n",
    "base_dir = \"runs\"\n",
    "\n",
    "# Name of the experiment\n",
    "experiment_name = \"your_experiment_name\"\n",
    "\n",
    "# Get the next version number for the experiment\n",
    "next_version_number = get_next_version_number(base_dir, experiment_name)\n",
    "\n",
    "log_dir = os.path.join(base_dir, experiment_name, f\"version_{next_version_number}\")\n",
    "\n",
    "# Initialize the TensorBoard writer with the unique log directory\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "overfit = False\n",
    "\n",
    "for i in tqdm(range(epoch)):\n",
    "    train_loop(\n",
    "        train_loader,\n",
    "        model,\n",
    "        loss_weights,\n",
    "        initial_task_losses,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        tr_model_specification,\n",
    "        epoch_num=i,\n",
    "        overfit=overfit,\n",
    "    )\n",
    "\n",
    "    val_loop(\n",
    "        val_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        device,\n",
    "        val_model_specification,\n",
    "        epoch_num=i,\n",
    "        overfit=overfit,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242737e-6959-4f6e-91e7-50c32c10f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = torch.rand((10, 3, 224, 224))\n",
    "\n",
    "model = ResNet50()\n",
    "x = model(sample_data, \"regression\")\n",
    "\n",
    "print(sample_data.size())\n",
    "print(x.size())\n",
    "\n",
    "x = model(sample_data, \"classification\")\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e6d63-319d-4fe7-8b60-8b9b82ee8e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch, batch_id, dataloader_idx in train_loader:\n",
    "    print(f\"{batch_id = }, {dataloader_idx = }\")\n",
    "\n",
    "    for key, value in batch.items():\n",
    "        x, y = value\n",
    "        print(f\"{key}: {x.size()}, {y.size()}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
