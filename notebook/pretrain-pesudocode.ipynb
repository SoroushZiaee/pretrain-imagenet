{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ligthning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor, StochasticWeightAveraging\n",
    "\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from lit_modules import LitAlexNet, TinyImageNetDataModule, AlexNet, CifarDataModule\n",
    "\n",
    "from lightning import Trainer\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming CifarDataModule and AlexNet are defined and imported correctly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# datamodule = CifarDataModule(batch_size=32)\n",
    "data_path = \"../data/tiny-imagenet/tiny-imagenet-200/train\"\n",
    "\n",
    "datamodule = TinyImageNetDataModule(data_path=data_path, batch_size=128)\n",
    "datamodule.setup(\"train\")\n",
    "\n",
    "ld = datamodule.train_dataloader()\n",
    "temp_ld = [list(next(iter(ld)))]\n",
    "\n",
    "print(f\"{len(temp_ld) = }\")\n",
    "\n",
    "alexnet = AlexNet(num_classes=1000)\n",
    "alexnet.to(device)\n",
    "\n",
    "print(f\"{alexnet.features[3].bias}\")\n",
    "plt.hist(alexnet.features[3].bias.cpu().detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "# You must enable gradients by calling `requires_grad_()` on the parameters if it's not already done\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad_()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "acc_top_1 = Accuracy(task=\"multiclass\", num_classes=1000, top_k=1)\n",
    "acc_top_1.to(device)\n",
    "\n",
    "acc_top_5 = Accuracy(task=\"multiclass\", num_classes=1000, top_k=5)\n",
    "acc_top_5.to(device)\n",
    "\n",
    "# Assume 'optimizer' is defined (e.g., Adam, SGD, etc.)\n",
    "optimizer = torch.optim.Adam(alexnet.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.SGD(alexnet.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "losses = []\n",
    "acc_1 = []\n",
    "acc_5 = []\n",
    "\n",
    "for _ in tqdm(range(500)):\n",
    "    # for i, (x, y) in tqdm(enumerate(temp_ld)):\n",
    "    for i, (x, y) in enumerate(temp_ld):\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # print(f\"{x.size() = }\")\n",
    "        # print(f\"{y.size() = }\")\n",
    "        # print(f\"{y = }\")\n",
    "    \n",
    "        # Forward pass: Compute predicted outputs by passing inputs to the model\n",
    "        outputs = alexnet(x)\n",
    "        # print(f\"{outputs.size() = }\")\n",
    "        # print(f\"{outputs = }\")\n",
    "    \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, y)\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "        # print(f\"{acc_top_1(F.softmax(outputs, dim=1), y) = }\")\n",
    "        accuracy_1 = acc_top_1(F.softmax(outputs, dim=1), y).item()\n",
    "        accuracy_5 = acc_top_5(F.softmax(outputs, dim=1), y).item()\n",
    "        \n",
    "        acc_1.append(accuracy_1)\n",
    "        acc_5.append(accuracy_5)\n",
    "        \n",
    "    \n",
    "        # Zero the gradients before running the backward pass.\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "    \n",
    "        # Perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Check the gradients for the first batch\n",
    "        # if i == 0:\n",
    "        #     for name, parameter in alexnet.named_parameters():\n",
    "        #         if parameter.requires_grad:\n",
    "        #             print(f\"{name} - gradient: {parameter.grad}\")\n",
    "    \n",
    "        # Visualize the images in the second batch\n",
    "        if i == 1:\n",
    "            # temp = x[0].permute(1, 2, 0).detach().numpy()\n",
    "            # plt.imshow(temp)\n",
    "            # plt.show()\n",
    "            # temp = x[1].permute(1, 2, 0).detach().numpy()\n",
    "            # plt.imshow(temp)\n",
    "            # plt.show()\n",
    "    \n",
    "            pass\n",
    "    \n",
    "        # Break after the second batch\n",
    "        if i == 10:\n",
    "            break\n",
    "\n",
    "print(f\"{alexnet.features[3].bias}\")\n",
    "print(f\"{alexnet.features[3].bias.grad}\")\n",
    "\n",
    "\n",
    "plt.hist(alexnet.features[3].bias.cpu().detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(acc_1)\n",
    "plt.plot(acc_5)\n",
    "plt.legend([\"Top-1\", \"Top-2\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments \n",
    "\n",
    "\n",
    "- Version0\n",
    "    - Overfit on the single batch\n",
    "- Version1\n",
    "    - Training on all dataset\n",
    "- Version2\n",
    "    - Experiment:\n",
    "        - Add Early Stoppping\n",
    "        - Add LR scheduler\n",
    "        - Remove StochasticWeightAveraging(swa_lrs=1e-2)\n",
    "        - Add Learning Rate Logging to tensorboard on each Epoch\n",
    "    - Result:\n",
    "        - The Model stop at Epoch 9\n",
    "        - Validation Top1_Err = 36%\n",
    "        - Validation Top5_Err = 5%\n",
    "        - Train Loss = 0.7368\n",
    "        - Val Loss = 1.174\n",
    " \n",
    "---\n",
    "- Version0\n",
    "    - Expriment:\n",
    "        - Correct the learning_rate of Adam optimizer\n",
    "        - Change the EarlyStopping setting EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10, check_val_every_n_epoch=3)\n",
    "        - Set the Learning Rate = 0.01\n",
    "    - Result\n",
    "        - Training isn't starting\n",
    "- Version1\n",
    "    - Experiment:\n",
    "        - Set the Learning Rate = 0.001\n",
    "    - Result:\n",
    "        - Val Top1_Err = 32%\n",
    "        - Val Top5_Err = 4.6%\n",
    "        - Train Loss = 0.2\n",
    "        - Val Loss = 1\n",
    "- Version2\n",
    "    - Experiment:\n",
    "        - Test overfitting On one Batch\n",
    "    - Result:\n",
    "        - Train Loss = 2\n",
    "     \n",
    "- Version3\n",
    "    - Experiment:\n",
    "        - Test overfitting On one Batch\n",
    "        - Set the learning rate to 0.01\n",
    "    - Result:\n",
    "        - Train Loss = 2\n",
    "\n",
    "- Version4\n",
    "    - Experiment:\n",
    "        - Adam to SGD\n",
    "        - Test overfitting On one Batch\n",
    "        - Set the learning rate to 0.01\n",
    "    - Result:\n",
    "        - Train Loss = 2\n",
    "\n",
    "- Version5\n",
    "    - Experiment:\n",
    "        - Adam to SGD\n",
    "        - Test overfitting On one Batch\n",
    "        - Set the learning rate to 0.001\n",
    "    - Result:\n",
    "        - Train Loss = 2\n",
    "     \n",
    "- Version6\n",
    "    - Experiment:\n",
    "        - Adam to SGD\n",
    "        - Test overfitting On one Batch\n",
    "        - Set the learning rate to 0.003\n",
    "    - Result:\n",
    "        - Train Loss = 2\n",
    "\n",
    "- Version7\n",
    "    - Experiment:\n",
    "        - Adam to SGD\n",
    "        - Test overfitting On one Batch\n",
    "        - Set the learning rate to 0.1\n",
    "    - Result:\n",
    "        - Train Loss = 2\n",
    "\n",
    "- Version8\n",
    "    - Experiment:\n",
    "        - Adam to SGD\n",
    "        - Test overfitting On one Batch\n",
    "        - Set the learning rate to 0.0001\n",
    "    - Result:\n",
    "        - Train Loss = 2\n",
    "\n",
    "- Version9\n",
    "    - Experiment:\n",
    "        - Adam\n",
    "        - Test overfitting On one Batch\n",
    "        - Set the learning rate to 0.001\n",
    "    - Result:\n",
    "        - Train Loss = 2\n",
    "     \n",
    "- Version10\n",
    "    - Experiment:\n",
    "        - Adam\n",
    "        - Remove the Learning rate scheduler\n",
    "        - StochasticWeightAveraging(swa_lrs=1e-2)\n",
    "        - Test overfitting On one Batch\n",
    "        - Set the learning rate to 0.001\n",
    "    - Result:\n",
    "        - Train Loss = 2\n",
    "\n",
    "- Version11\n",
    "    - Experiment:\n",
    "        - Adam\n",
    "        - Remove the Learning rate scheduler\n",
    "        - StochasticWeightAveraging(swa_lrs=1e-2)\n",
    "        - Test overfitting On one Batch\n",
    "        - Set the learning rate to 0.01\n",
    "    - Result:\n",
    "        - Train Loss = 2\n",
    "     \n",
    "- Version12\n",
    "    - Experiment:\n",
    "        - Adam\n",
    "        - Remove the Learning rate scheduler\n",
    "        - StochasticWeightAveraging(swa_lrs=1e-2)\n",
    "        - Test overfitting On one Batch\n",
    "        - Set the learning rate to 0.003\n",
    "    - Result:\n",
    "        - Epoch = 125\n",
    "        - Train Loss = 0.07\n",
    "\n",
    "- Version13\n",
    "    - Experiment:\n",
    "        - Batch Finder And Learning Rate Finder\n",
    "    - Results:\n",
    "        - Batchsize = 32K\n",
    "        - Learning Rate = 1e-3\n",
    "\n",
    "- Version14\n",
    "    - Experiment:\n",
    "        - Set the learning rate to 0.003\n",
    "        - Batch size = 512\n",
    "        - Learning Rate = 3e-3\n",
    "    - Results:\n",
    "        - Not Working\n",
    "\n",
    "- Version14\n",
    "    - Experiment:\n",
    "        - Set the learning rate to 0.003\n",
    "        - Batch size = 256\n",
    "        - Learning Rate = 3e-3\n",
    "    - Results:\n",
    "\n",
    "- Version15\n",
    "    - Experiment:\n",
    "        - Set the learning rate to 0.003\n",
    "        - Batch size = 256\n",
    "    - Results:\n",
    "        - Val Top1 = 33%\n",
    "        - Val Top5 = 5%\n",
    "        - Train Loss = 0.7\n",
    "\n",
    "- Version16\n",
    "    - Experiment:\n",
    "        - Set the learning rate to 0.003\n",
    "        - Batch size = 256\n",
    "        - set dropout rate = 0.5\n",
    "        - add weight decay = 1e-4\n",
    "    - Results:\n",
    "        - The best model ever is made\n",
    " \n",
    "---\n",
    "- Version17\n",
    "    - Experiment:\n",
    "        - ImageNet\n",
    "        - Overfit Test\n",
    "    - Results:\n",
    "        - The best model ever is made\n",
    "\n",
    "- Version18\n",
    "    - Experiment:\n",
    "        - ImageNet\n",
    "        - Overfit Test\n",
    "        - LR = 0.01\n",
    "    - Results:\n",
    "        - \n",
    "\n",
    "- Version19\n",
    "    - Experiment:\n",
    "        - ImageNet\n",
    "        - Overfit Test\n",
    "        - LR = 0.001\n",
    "        - weight_decay=5e-4\n",
    "    - Results:\n",
    "        - The best model ever is made\n",
    "\n",
    "- Version20\n",
    "    - Experiment:\n",
    "        - ImageNet\n",
    "        - Overfit Test\n",
    "        - LR = 0.001\n",
    "        - weight_decay=5e-4\n",
    "    - Results:\n",
    "        - shit\n",
    "\n",
    "- Version21\n",
    "    - Experiment:\n",
    "        - ImageNet\n",
    "        - Overfit Test\n",
    "        - LR = 0.01\n",
    "        - weight_decay=5e-4\n",
    "    - Results:\n",
    "        - shit\n",
    "     \n",
    "- Version21\n",
    "    - Experiment:\n",
    "        - ImageNet\n",
    "        - Overfit Test\n",
    "        - LR = 0.01\n",
    "        - weight_decay=1e-4\n",
    "    - Results:\n",
    "        - shit\n",
    "\n",
    "- Version22\n",
    "    - Experiment:\n",
    "        - ImageNet\n",
    "        - Overfit Test\n",
    "        - LR = 0.001\n",
    "        - weight_decay=1e-4\n",
    "    - Results:\n",
    "        - shit\n",
    "\n",
    "---\n",
    "\n",
    "- Version22\n",
    "    - Experiment:\n",
    "        - ImageNet\n",
    "        - Overfit Test\n",
    "        - Add new initialization Kaiman He\n",
    "        - Add BatchNorm after each layer\n",
    "        - Using ELU activation instead of RELU (Because the ELU has the value for negative elements)\n",
    "    - Results:\n",
    "        - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/tiny-imagenet/tiny-imagenet-200/train\"\n",
    "datamodule = TinyImageNetDataModule(data_path=data_path, batch_size=96)\n",
    "# datamodule = CifarDataModule(batch_size=256)\n",
    "\n",
    "\n",
    "model = LitAlexNet(learning_rate=1e-3, num_classes=1000, example_input_array=(96, 3, 224, 224))\n",
    "tb_logger = TensorBoardLogger('.')\n",
    "\n",
    "trainer = Trainer(\n",
    "        max_epochs=200,\n",
    "        # callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "        callbacks=[StochasticWeightAveraging(swa_lrs=1e-2)],\n",
    "        fast_dev_run=False,\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"gpu\",\n",
    "        num_nodes=1,\n",
    "        strategy=\"auto\",\n",
    "        # overfit_batches=1,\n",
    "        gradient_clip_val=0.5,\n",
    "        logger=tb_logger,\n",
    "        # check_val_every_n_epoch = 1,\n",
    "        # log_every_n_steps=1\n",
    "    )\n",
    "\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/tiny-imagenet/tiny-imagenet-200/train\"\n",
    "# datamodule = TinyImageNetDataModule(data_path=data_path, batch_size=96)\n",
    "datamodule = CifarDataModule(batch_size=96)\n",
    "\n",
    "\n",
    "model = LitAlexNet(learning_rate=3e-3, num_classes=10, example_input_array=(96, 3, 224, 224))\n",
    "tb_logger = TensorBoardLogger('.')\n",
    "\n",
    "trainer = Trainer(\n",
    "        max_epochs=200,\n",
    "        # callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)],\n",
    "        callbacks=[StochasticWeightAveraging(swa_lrs=1e-2)],\n",
    "        fast_dev_run=False,\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"gpu\",\n",
    "        num_nodes=1,\n",
    "        strategy=\"auto\",\n",
    "        overfit_batches=1,\n",
    "        gradient_clip_val=0.5,\n",
    "        logger=tb_logger,\n",
    "        # check_val_every_n_epoch = 1,\n",
    "        # log_every_n_steps=1\n",
    "    )\n",
    "\n",
    "tuner = Tuner(trainer)\n",
    "# Auto-scale batch size by growing it exponentially (default)\n",
    "tuner.scale_batch_size(model, datamodule=datamodule, mode=\"power\")\n",
    "\n",
    "# finds learning rate automatically\n",
    "# sets hparams.lr or hparams.learning_rate to that learning rate\n",
    "# Run learning rate finder\n",
    "lr_finder = tuner.lr_find(model, datamodule=datamodule)\n",
    "\n",
    "# Results can be found in\n",
    "print(lr_finder.results)\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "new_lr = lr_finder.suggestion()\n",
    "\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "\n",
    "# update hparams of the model\n",
    "model.hparams.learning_rate = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()\n",
    "\n",
    "!python ../main.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
